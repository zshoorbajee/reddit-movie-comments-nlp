{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC, RootMeanSquaredError, mean_absolute_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "from transformers import pipelines\n",
    "\n",
    "import re\n",
    "from collections import OrderedDict, Counter\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import praw\n",
    "\n",
    "import json\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.max_rows = 100\n",
    "seed = 55\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/data_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tconst</th>\n",
       "      <th>title</th>\n",
       "      <th>originalTitle</th>\n",
       "      <th>comments</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>startYear</th>\n",
       "      <th>post_date_utc</th>\n",
       "      <th>post_year</th>\n",
       "      <th>post_month</th>\n",
       "      <th>post_day</th>\n",
       "      <th>genres</th>\n",
       "      <th>numVotes</th>\n",
       "      <th>averageRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vzcwal</td>\n",
       "      <td>tt13406136</td>\n",
       "      <td>the princess</td>\n",
       "      <td>The Princess</td>\n",
       "      <td>Joey King needs a new agent. She’s proven she has talent but she has so many terrible films on h...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.657851e+09</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>Action,Drama,Fantasy</td>\n",
       "      <td>11474</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vzcwal</td>\n",
       "      <td>tt13406136</td>\n",
       "      <td>the princess</td>\n",
       "      <td>The Princess</td>\n",
       "      <td>Silly, but entertaining and non stop action</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.657851e+09</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>Action,Drama,Fantasy</td>\n",
       "      <td>11474</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vzcwal</td>\n",
       "      <td>tt13406136</td>\n",
       "      <td>the princess</td>\n",
       "      <td>The Princess</td>\n",
       "      <td>The yassification of The Raid\\n\\nActually, this was fun enough and mad respect to Joey King for ...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.657851e+09</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>Action,Drama,Fantasy</td>\n",
       "      <td>11474</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vzcwal</td>\n",
       "      <td>tt13406136</td>\n",
       "      <td>the princess</td>\n",
       "      <td>The Princess</td>\n",
       "      <td>Honestly, this was pretty fun.  The plot is nothing special yes.\\n\\nBut Joey King was actually e...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.657851e+09</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>Action,Drama,Fantasy</td>\n",
       "      <td>11474</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vzcwal</td>\n",
       "      <td>tt13406136</td>\n",
       "      <td>the princess</td>\n",
       "      <td>The Princess</td>\n",
       "      <td>Man, I loved this movie. Yeah, it was campy, but whatever. The premise worked for me, I liked th...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.657851e+09</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>Action,Drama,Fantasy</td>\n",
       "      <td>11474</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      tconst         title originalTitle  \\\n",
       "0  vzcwal  tt13406136  the princess  The Princess   \n",
       "1  vzcwal  tt13406136  the princess  The Princess   \n",
       "2  vzcwal  tt13406136  the princess  The Princess   \n",
       "3  vzcwal  tt13406136  the princess  The Princess   \n",
       "4  vzcwal  tt13406136  the princess  The Princess   \n",
       "\n",
       "                                                                                              comments  \\\n",
       "0  Joey King needs a new agent. She’s proven she has talent but she has so many terrible films on h...   \n",
       "1                                                          Silly, but entertaining and non stop action   \n",
       "2  The yassification of The Raid\\n\\nActually, this was fun enough and mad respect to Joey King for ...   \n",
       "3  Honestly, this was pretty fun.  The plot is nothing special yes.\\n\\nBut Joey King was actually e...   \n",
       "4  Man, I loved this movie. Yeah, it was campy, but whatever. The premise worked for me, I liked th...   \n",
       "\n",
       "   runtimeMinutes  startYear  post_date_utc  post_year  post_month  post_day  \\\n",
       "0            94.0       2022   1.657851e+09       2022           7        14   \n",
       "1            94.0       2022   1.657851e+09       2022           7        14   \n",
       "2            94.0       2022   1.657851e+09       2022           7        14   \n",
       "3            94.0       2022   1.657851e+09       2022           7        14   \n",
       "4            94.0       2022   1.657851e+09       2022           7        14   \n",
       "\n",
       "                 genres  numVotes  averageRating  \n",
       "0  Action,Drama,Fantasy     11474            5.6  \n",
       "1  Action,Drama,Fantasy     11474            5.6  \n",
       "2  Action,Drama,Fantasy     11474            5.6  \n",
       "3  Action,Drama,Fantasy     11474            5.6  \n",
       "4  Action,Drama,Fantasy     11474            5.6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imploded  = df.groupby(['id', 'title', 'averageRating'])[['comments']].agg(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imploded['comments'] = df_imploded['comments'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imploded['comments'] = df_imploded['comments'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imploded['comments'] = df_imploded['comments'].apply(\n",
    "    lambda x: re.sub(\n",
    "        pattern=r'http\\S+', repl='HYPERLINK', string=x)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = r\"[a-zA-Z]+'?[a-zA-Z]+|\\b[iIaA]\\b\"\n",
    "# Pattern: Any word with at least two characters, including up to one apostrophe\n",
    "# Also captures the English words \"I\" and \"a\".\n",
    "\n",
    "tokenizer = RegexpTokenizer(token_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_spacy = list(nlp.Defaults.stop_words)\n",
    "sw_nltk = stopwords.words('english')\n",
    "stopword_list = list(set(sw_spacy + sw_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list_stem = [stemmer.stem(sw) for sw in stopword_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imploded['comments_tok'] = df_imploded['comments'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imploded['comments_tok_sw'] = df_imploded['comments_tok'].apply(lambda x: [t for t in x if t not in stopword_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imploded['comments_stem'] = df_imploded['comments_tok'].apply(lambda x: [stemmer.stem(t) for t in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imploded['comments_stem_sw'] = df_imploded['comments_stem'].apply(lambda x: [t for t in x if t not in stopword_list_stem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>averageRating</th>\n",
       "      <th>comments</th>\n",
       "      <th>comments_tok</th>\n",
       "      <th>comments_tok_sw</th>\n",
       "      <th>comments_stem</th>\n",
       "      <th>comments_stem_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47szbr</td>\n",
       "      <td>crouching tiger, hidden dragon: sword of destiny</td>\n",
       "      <td>6.1</td>\n",
       "      <td>just finished this, and i thought it was ok. compared to the first it seemed a lot more, i don't...</td>\n",
       "      <td>[just, finished, this, and, i, thought, it, was, ok, compared, to, the, first, it, seemed, a, lo...</td>\n",
       "      <td>[finished, thought, ok, compared, lot, know, cheesy, sorta, fun, way, glad, cast, moving, wu, da...</td>\n",
       "      <td>[just, finish, this, and, i, thought, it, was, ok, compar, to, the, first, it, seem, a, lot, mor...</td>\n",
       "      <td>[finish, thought, ok, compar, lot, know, cheesi, sorta, fun, way, glad, cast, wu, dang, mountain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48vhc8</td>\n",
       "      <td>zootopia</td>\n",
       "      <td>8.0</td>\n",
       "      <td>\"hold on, walter and jesse are at the door.\"\\n\\ni thought that scene felt similar to breaking ba...</td>\n",
       "      <td>[hold, on, walter, and, jesse, are, at, the, door, i, thought, that, scene, felt, similar, to, b...</td>\n",
       "      <td>[hold, walter, jesse, door, thought, scene, felt, similar, breaking, bad, consider, zootopia, sm...</td>\n",
       "      <td>[hold, on, walter, and, jess, are, at, the, door, i, thought, that, scene, felt, similar, to, br...</td>\n",
       "      <td>[hold, walter, jess, door, thought, scene, felt, similar, break, bad, consid, zootopia, smartest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48vhmk</td>\n",
       "      <td>whiskey tango foxtrot</td>\n",
       "      <td>6.6</td>\n",
       "      <td>i personally enjoyed it. not the best, not the worst, but i liked it. i liked the way they handl...</td>\n",
       "      <td>[i, personally, enjoyed, it, not, the, best, not, the, worst, but, i, liked, it, i, liked, the, ...</td>\n",
       "      <td>[personally, enjoyed, best, worst, liked, liked, way, handled, humor, especially, genre, martin,...</td>\n",
       "      <td>[i, person, enjoy, it, not, the, best, not, the, worst, but, i, like, it, i, like, the, way, the...</td>\n",
       "      <td>[person, enjoy, best, worst, like, like, way, handl, humor, especi, genr, martin, freeman, scruf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48vhsf</td>\n",
       "      <td>london has fallen</td>\n",
       "      <td>5.9</td>\n",
       "      <td>if you go into this movie expecting good action, gerard butler being a badass, and a terrible pl...</td>\n",
       "      <td>[if, you, go, into, this, movie, expecting, good, action, gerard, butler, being, a, badass, and,...</td>\n",
       "      <td>[movie, expecting, good, action, gerard, butler, badass, terrible, plot, bases, covered, kind, m...</td>\n",
       "      <td>[if, you, go, into, this, movi, expect, good, action, gerard, butler, be, a, badass, and, a, ter...</td>\n",
       "      <td>[movi, expect, good, action, gerard, butler, badass, terribl, plot, base, cover, kind, movi, com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49wvnj</td>\n",
       "      <td>10 cloverfield lane</td>\n",
       "      <td>7.2</td>\n",
       "      <td>i kept going back and forth: \"is he crazy?  no, he's right.  no he's crazy.  holy shit he's both...</td>\n",
       "      <td>[i, kept, going, back, and, forth, is, he, crazy, no, he's, right, no, he's, crazy, holy, shit, ...</td>\n",
       "      <td>[kept, going, forth, crazy, he's, right, he's, crazy, holy, shit, he's, crazy, right, edge, seat...</td>\n",
       "      <td>[i, kept, go, back, and, forth, is, he, crazi, no, he, right, no, he, crazi, holi, shit, he, bot...</td>\n",
       "      <td>[kept, forth, crazi, right, crazi, holi, shit, crazi, right, edg, seat, movi, essenti, hour, lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>vzcv66</td>\n",
       "      <td>where the crawdads sing</td>\n",
       "      <td>7.1</td>\n",
       "      <td>i did enjoy her house representing the 2 different ways the men treated her . tate was invited o...</td>\n",
       "      <td>[i, did, enjoy, her, house, representing, the, different, ways, the, men, treated, her, tate, wa...</td>\n",
       "      <td>[enjoy, house, representing, different, ways, men, treated, tate, invited, enter, invited, obser...</td>\n",
       "      <td>[i, did, enjoy, her, hous, repres, the, differ, way, the, men, treat, her, tate, was, invit, ove...</td>\n",
       "      <td>[enjoy, hous, repres, differ, way, men, treat, tate, invit, enter, invit, observ, respect, chase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>vzcvkz</td>\n",
       "      <td>mrs harris goes to paris</td>\n",
       "      <td>7.1</td>\n",
       "      <td>this was so cute it just made me smile the whole time.  highly recommend. the only word for this...</td>\n",
       "      <td>[this, was, so, cute, it, just, made, me, smile, the, whole, time, highly, recommend, the, only,...</td>\n",
       "      <td>[cute, smile, time, highly, recommend, word, movie, sweet, lovely, tiny, french, pastry, film, w...</td>\n",
       "      <td>[this, was, so, cute, it, just, made, me, smile, the, whole, time, high, recommend, the, onli, w...</td>\n",
       "      <td>[cute, smile, time, high, recommend, word, movi, sweet, love, tini, french, pastri, film, want, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>vzcvsd</td>\n",
       "      <td>the sea beast</td>\n",
       "      <td>7.1</td>\n",
       "      <td>absolutely crazy that netflix dropped this and also the mitchells vs the machines with almost no...</td>\n",
       "      <td>[absolutely, crazy, that, netflix, dropped, this, and, also, the, mitchells, vs, the, machines, ...</td>\n",
       "      <td>[absolutely, crazy, netflix, dropped, mitchells, vs, machines, fanfare, movies, incredible, anim...</td>\n",
       "      <td>[absolut, crazi, that, netflix, drop, this, and, also, the, mitchel, vs, the, machin, with, almo...</td>\n",
       "      <td>[absolut, crazi, netflix, drop, mitchel, vs, machin, fanfar, movi, incred, anim, great, stori, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>vzcw0a</td>\n",
       "      <td>the man from toronto</td>\n",
       "      <td>5.8</td>\n",
       "      <td>o offence to woody but i feel like the original casting of jason statham would have at least im...</td>\n",
       "      <td>[offence, to, woody, but, i, feel, like, the, original, casting, of, jason, statham, would, have...</td>\n",
       "      <td>[offence, woody, feel, like, original, casting, jason, statham, improved, shitxhow, slightly, ea...</td>\n",
       "      <td>[offenc, to, woodi, but, i, feel, like, the, origin, cast, of, jason, statham, would, have, at, ...</td>\n",
       "      <td>[offenc, woodi, feel, like, origin, cast, jason, statham, improv, shitxhow, slight, easi, tell, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>vzcwal</td>\n",
       "      <td>the princess</td>\n",
       "      <td>5.6</td>\n",
       "      <td>joey king needs a new agent. she’s proven she has talent but she has so many terrible films on h...</td>\n",
       "      <td>[joey, king, needs, a, new, agent, she, proven, she, has, talent, but, she, has, so, many, terri...</td>\n",
       "      <td>[joey, king, needs, new, agent, proven, talent, terrible, films, resume, silly, entertaining, no...</td>\n",
       "      <td>[joey, king, need, a, new, agent, she, proven, she, has, talent, but, she, has, so, mani, terrib...</td>\n",
       "      <td>[joey, king, need, new, agent, proven, talent, terribl, film, resum, silli, entertain, non, stop...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>922 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             title  averageRating  \\\n",
       "0    47szbr  crouching tiger, hidden dragon: sword of destiny            6.1   \n",
       "1    48vhc8                                          zootopia            8.0   \n",
       "2    48vhmk                             whiskey tango foxtrot            6.6   \n",
       "3    48vhsf                                 london has fallen            5.9   \n",
       "4    49wvnj                               10 cloverfield lane            7.2   \n",
       "..      ...                                               ...            ...   \n",
       "917  vzcv66                           where the crawdads sing            7.1   \n",
       "918  vzcvkz                          mrs harris goes to paris            7.1   \n",
       "919  vzcvsd                                     the sea beast            7.1   \n",
       "920  vzcw0a                              the man from toronto            5.8   \n",
       "921  vzcwal                                      the princess            5.6   \n",
       "\n",
       "                                                                                                comments  \\\n",
       "0    just finished this, and i thought it was ok. compared to the first it seemed a lot more, i don't...   \n",
       "1    \"hold on, walter and jesse are at the door.\"\\n\\ni thought that scene felt similar to breaking ba...   \n",
       "2    i personally enjoyed it. not the best, not the worst, but i liked it. i liked the way they handl...   \n",
       "3    if you go into this movie expecting good action, gerard butler being a badass, and a terrible pl...   \n",
       "4    i kept going back and forth: \"is he crazy?  no, he's right.  no he's crazy.  holy shit he's both...   \n",
       "..                                                                                                   ...   \n",
       "917  i did enjoy her house representing the 2 different ways the men treated her . tate was invited o...   \n",
       "918  this was so cute it just made me smile the whole time.  highly recommend. the only word for this...   \n",
       "919  absolutely crazy that netflix dropped this and also the mitchells vs the machines with almost no...   \n",
       "920   o offence to woody but i feel like the original casting of jason statham would have at least im...   \n",
       "921  joey king needs a new agent. she’s proven she has talent but she has so many terrible films on h...   \n",
       "\n",
       "                                                                                            comments_tok  \\\n",
       "0    [just, finished, this, and, i, thought, it, was, ok, compared, to, the, first, it, seemed, a, lo...   \n",
       "1    [hold, on, walter, and, jesse, are, at, the, door, i, thought, that, scene, felt, similar, to, b...   \n",
       "2    [i, personally, enjoyed, it, not, the, best, not, the, worst, but, i, liked, it, i, liked, the, ...   \n",
       "3    [if, you, go, into, this, movie, expecting, good, action, gerard, butler, being, a, badass, and,...   \n",
       "4    [i, kept, going, back, and, forth, is, he, crazy, no, he's, right, no, he's, crazy, holy, shit, ...   \n",
       "..                                                                                                   ...   \n",
       "917  [i, did, enjoy, her, house, representing, the, different, ways, the, men, treated, her, tate, wa...   \n",
       "918  [this, was, so, cute, it, just, made, me, smile, the, whole, time, highly, recommend, the, only,...   \n",
       "919  [absolutely, crazy, that, netflix, dropped, this, and, also, the, mitchells, vs, the, machines, ...   \n",
       "920  [offence, to, woody, but, i, feel, like, the, original, casting, of, jason, statham, would, have...   \n",
       "921  [joey, king, needs, a, new, agent, she, proven, she, has, talent, but, she, has, so, many, terri...   \n",
       "\n",
       "                                                                                         comments_tok_sw  \\\n",
       "0    [finished, thought, ok, compared, lot, know, cheesy, sorta, fun, way, glad, cast, moving, wu, da...   \n",
       "1    [hold, walter, jesse, door, thought, scene, felt, similar, breaking, bad, consider, zootopia, sm...   \n",
       "2    [personally, enjoyed, best, worst, liked, liked, way, handled, humor, especially, genre, martin,...   \n",
       "3    [movie, expecting, good, action, gerard, butler, badass, terrible, plot, bases, covered, kind, m...   \n",
       "4    [kept, going, forth, crazy, he's, right, he's, crazy, holy, shit, he's, crazy, right, edge, seat...   \n",
       "..                                                                                                   ...   \n",
       "917  [enjoy, house, representing, different, ways, men, treated, tate, invited, enter, invited, obser...   \n",
       "918  [cute, smile, time, highly, recommend, word, movie, sweet, lovely, tiny, french, pastry, film, w...   \n",
       "919  [absolutely, crazy, netflix, dropped, mitchells, vs, machines, fanfare, movies, incredible, anim...   \n",
       "920  [offence, woody, feel, like, original, casting, jason, statham, improved, shitxhow, slightly, ea...   \n",
       "921  [joey, king, needs, new, agent, proven, talent, terrible, films, resume, silly, entertaining, no...   \n",
       "\n",
       "                                                                                           comments_stem  \\\n",
       "0    [just, finish, this, and, i, thought, it, was, ok, compar, to, the, first, it, seem, a, lot, mor...   \n",
       "1    [hold, on, walter, and, jess, are, at, the, door, i, thought, that, scene, felt, similar, to, br...   \n",
       "2    [i, person, enjoy, it, not, the, best, not, the, worst, but, i, like, it, i, like, the, way, the...   \n",
       "3    [if, you, go, into, this, movi, expect, good, action, gerard, butler, be, a, badass, and, a, ter...   \n",
       "4    [i, kept, go, back, and, forth, is, he, crazi, no, he, right, no, he, crazi, holi, shit, he, bot...   \n",
       "..                                                                                                   ...   \n",
       "917  [i, did, enjoy, her, hous, repres, the, differ, way, the, men, treat, her, tate, was, invit, ove...   \n",
       "918  [this, was, so, cute, it, just, made, me, smile, the, whole, time, high, recommend, the, onli, w...   \n",
       "919  [absolut, crazi, that, netflix, drop, this, and, also, the, mitchel, vs, the, machin, with, almo...   \n",
       "920  [offenc, to, woodi, but, i, feel, like, the, origin, cast, of, jason, statham, would, have, at, ...   \n",
       "921  [joey, king, need, a, new, agent, she, proven, she, has, talent, but, she, has, so, mani, terrib...   \n",
       "\n",
       "                                                                                        comments_stem_sw  \n",
       "0    [finish, thought, ok, compar, lot, know, cheesi, sorta, fun, way, glad, cast, wu, dang, mountain...  \n",
       "1    [hold, walter, jess, door, thought, scene, felt, similar, break, bad, consid, zootopia, smartest...  \n",
       "2    [person, enjoy, best, worst, like, like, way, handl, humor, especi, genr, martin, freeman, scruf...  \n",
       "3    [movi, expect, good, action, gerard, butler, badass, terribl, plot, base, cover, kind, movi, com...  \n",
       "4    [kept, forth, crazi, right, crazi, holi, shit, crazi, right, edg, seat, movi, essenti, hour, lon...  \n",
       "..                                                                                                   ...  \n",
       "917  [enjoy, hous, repres, differ, way, men, treat, tate, invit, enter, invit, observ, respect, chase...  \n",
       "918  [cute, smile, time, high, recommend, word, movi, sweet, love, tini, french, pastri, film, want, ...  \n",
       "919  [absolut, crazi, netflix, drop, mitchel, vs, machin, fanfar, movi, incred, anim, great, stori, w...  \n",
       "920  [offenc, woodi, feel, like, origin, cast, jason, statham, improv, shitxhow, slight, easi, tell, ...  \n",
       "921  [joey, king, need, new, agent, proven, talent, terribl, film, resum, silli, entertain, non, stop...  \n",
       "\n",
       "[922 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(figsize=(7, 24))\n",
    "# fd_0 = FreqDist(df['comments_stem_sw'].explode()).most_common(100)\n",
    "# fd_0 = OrderedDict(fd_0)\n",
    "# tokens_0 = list(fd_0.keys())[::-1]\n",
    "# freq_0 = list(fd_0.values())[::-1]\n",
    "# # fd_1 = FreqDist(series[y==1].explode()).most_common(cutoff)\n",
    "# # fd_1 = OrderedDict(fd_1)\n",
    "# # tokens_1 = list(fd_1.keys())[::-1]\n",
    "# # freq_1 = list(fd_1.values())[::-1]\n",
    "# # shared_tokens = [t for t in tokens_0 if t in tokens_1]\n",
    "# axes.barh(y=tokens_0, width=freq_0)\n",
    "# # axes[1].barh(y=tokens_1, width=freq_1, color=['C6' if token in shared_tokens else 'C0' for token in tokens_1])\n",
    "# axes.set_ylabel('Tokens', size=8)\n",
    "# axes.set_xlabel('Frequency', size=8)\n",
    "# # axes[1].set_xlabel('Frequency', size=10)\n",
    "# # fig.suptitle(f'Top {cutoff} {token_type}', size=15)\n",
    "# axes.set_title('Non-Disaster')\n",
    "# # axes[1].set_title('Disaster')\n",
    "# # custom_bars = [Line2D([0], [0], color='C6', lw=10), Line2D([0], [0], color='C0', lw=10)]\n",
    "# # axes.legend(custom_bars, ['In common', 'Not in common'])\n",
    "# # axes[1].legend(custom_bars, ['In common', 'Not in common'])\n",
    "# fig.set_facecolor('white');\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = df_imploded.drop(columns='averageRating')\n",
    "target = df_imploded['averageRating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zshoo\\anaconda3\\envs\\capstone-env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anywh', 'el', 'elsewh', 'everywh', 'ind', 'otherwi', 'plea', 'somewh'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(922, 1000)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lambda x: [stemmer.stem(t) for t in tokenizer.tokenize(x)], \n",
    "    stop_words=stopword_list_stem,\n",
    "    max_features=1000\n",
    "    )\n",
    "\n",
    "tfidf_mat = vectorizer.fit_transform(comments['comments'])\n",
    "tfidf_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best_indices(m, topk, mask=None):\n",
    "    \"\"\"\n",
    "    Use sum of the cosine distance over all tokens.\n",
    "    m (np.array): cos matrix of shape (nb_in_tokens, nb_dict_tokens)\n",
    "    topk (int): number of indices to return (from high to lowest in order)\n",
    "    \"\"\"\n",
    "    # return the sum on all tokens of cosinus for each sentence\n",
    "    if len(m.shape) > 1:\n",
    "        cos_sim = np.mean(m, axis=0) \n",
    "    else: \n",
    "        cos_sim = m\n",
    "    index = np.argsort(cos_sim)[::-1] # from highest idx to smallest score \n",
    "    if mask is not None:\n",
    "        assert mask.shape == m.shape\n",
    "        mask = mask[index]\n",
    "    else:\n",
    "        mask = np.ones(len(cos_sim))\n",
    "    mask = np.logical_or(cos_sim[index] != 0, mask) #eliminate 0 cosine distance\n",
    "    best_index = index[mask][:topk]  \n",
    "    return best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations_tfidf(sentence, tfidf_mat):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the database sentences in order of highest cosine similarity relatively to each \n",
    "    token of the target sentence. \n",
    "    \"\"\"\n",
    "    # Embed the query sentence\n",
    "    tokens = [stemmer.stem(t) for t in tokenizer.tokenize(sentence)]\n",
    "    vec = vectorizer.transform(tokens)\n",
    "    # Create list with similarity between query and dataset\n",
    "    mat = cosine_similarity(vec, tfidf_mat)\n",
    "    # Best cosine distance for each token independantly\n",
    "    print(mat.shape)\n",
    "    best_index = extract_best_indices(mat, topk=10)\n",
    "    return best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>averageRating</th>\n",
       "      <th>comments</th>\n",
       "      <th>comments_tok</th>\n",
       "      <th>comments_tok_sw</th>\n",
       "      <th>comments_stem</th>\n",
       "      <th>comments_stem_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>5rrasx</td>\n",
       "      <td>rings</td>\n",
       "      <td>4.5</td>\n",
       "      <td>priest: samara can't hurt me cuz i'm blind lol\\n\\nsamara: bet *the bye bye man* was at least fun...</td>\n",
       "      <td>[priest, samara, can't, hurt, me, cuz, i'm, blind, lol, samara, bet, the, bye, bye, man, was, at...</td>\n",
       "      <td>[priest, samara, can't, hurt, cuz, i'm, blind, lol, samara, bet, bye, bye, man, fun, ineptitude,...</td>\n",
       "      <td>[priest, samara, can't, hurt, me, cuz, i'm, blind, lol, samara, bet, the, bye, bye, man, was, at...</td>\n",
       "      <td>[priest, samara, can't, hurt, cuz, i'm, blind, lol, samara, bet, bye, bye, man, fun, ineptitud, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  title  averageRating  \\\n",
       "115  5rrasx  rings            4.5   \n",
       "\n",
       "                                                                                                comments  \\\n",
       "115  priest: samara can't hurt me cuz i'm blind lol\\n\\nsamara: bet *the bye bye man* was at least fun...   \n",
       "\n",
       "                                                                                            comments_tok  \\\n",
       "115  [priest, samara, can't, hurt, me, cuz, i'm, blind, lol, samara, bet, the, bye, bye, man, was, at...   \n",
       "\n",
       "                                                                                         comments_tok_sw  \\\n",
       "115  [priest, samara, can't, hurt, cuz, i'm, blind, lol, samara, bet, bye, bye, man, fun, ineptitude,...   \n",
       "\n",
       "                                                                                           comments_stem  \\\n",
       "115  [priest, samara, can't, hurt, me, cuz, i'm, blind, lol, samara, bet, the, bye, bye, man, was, at...   \n",
       "\n",
       "                                                                                        comments_stem_sw  \n",
       "115  [priest, samara, can't, hurt, cuz, i'm, blind, lol, samara, bet, bye, bye, man, fun, ineptitud, ...  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imploded.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 922)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>averageRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>kate</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>jolt</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>extraction</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>proud mary</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>the night comes for us</td>\n",
       "      <td>6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>peppermint</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>cold pursuit</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>the princess</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>angel has fallen</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>the 355</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      title  averageRating\n",
       "795                    kate            6.2\n",
       "770                    jolt            5.6\n",
       "566              extraction            6.7\n",
       "223              proud mary            5.0\n",
       "319  the night comes for us            6.9\n",
       "300              peppermint            6.4\n",
       "375            cold pursuit            6.2\n",
       "921            the princess            5.6\n",
       "458        angel has fallen            6.4\n",
       "845                 the 355            5.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_sentence = 'superhero action crime drama dark night vengance'\n",
    "\n",
    "best_index = get_recommendations_tfidf(query_sentence, tfidf_mat)\n",
    "\n",
    "display(df_imploded[['title' , 'averageRating']].iloc[best_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the full dataset into training and testing data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    comments, target, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "# Splitting off a validation set\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=.5, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imploded['comments_stem'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imploded['comments'].sample().apply(lambda x: [stemmer.stem(t) for t in tokenizer.tokenize(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer=lambda x: [stemmer.stem(t) for t in tokenizer.tokenize(x)], \n",
    "    stop_words=stopword_list_stem,\n",
    "    max_features=1000\n",
    "    )\n",
    "\n",
    "X_train_vec = tfidf.fit_transform(X_train['comments'])\n",
    "\n",
    "X_train_vec_df = pd.DataFrame(\n",
    "    X_train_vec.toarray(),\n",
    "    columns=tfidf.get_feature_names_out(),\n",
    "    index=X_train.index\n",
    "    )\n",
    "X_train_vec_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_vec_df_scaled = scaler.fit_transform(X_train_vec_df)\n",
    "X_train_vec_df_scaled = pd.DataFrame(X_train_vec_df_scaled, index=X_train.index, columns=X_train_vec_df.columns)\n",
    "\n",
    "X_train_vec_df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vec = tfidf.transform(X_test['comments'])\n",
    "\n",
    "X_test_vec_df = pd.DataFrame(\n",
    "    X_test_vec.toarray(),\n",
    "    columns=tfidf.get_feature_names_out(),\n",
    "    index=X_test.index\n",
    "    )\n",
    "\n",
    "X_test_vec_df_scaled = scaler.transform(X_test_vec_df)\n",
    "X_test_vec_df_scaled = pd.DataFrame(X_test_vec_df_scaled, index=X_test.index, columns=X_test_vec_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_vec = tfidf.transform(X_val['comments'])\n",
    "\n",
    "X_val_vec_df = pd.DataFrame(\n",
    "    X_val_vec.toarray(),\n",
    "    columns=tfidf.get_feature_names_out(),\n",
    "    index=X_val.index\n",
    "    )\n",
    "\n",
    "X_val_vec_df_scaled = scaler.transform(X_val_vec_df)\n",
    "X_val_vec_df_scaled = pd.DataFrame(X_val_vec_df_scaled, index=X_val.index, columns=X_val_vec_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(np.array(y_train).reshape(-1,1))\n",
    "y_test_scaled = scaler_y.transform(np.array(y_test).reshape(-1,1))\n",
    "y_val_scaled = scaler_y.transform(np.array(y_val).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = X_train_vec_df_scaled.shape[1]\n",
    "n_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "model_baseline = models.Sequential()\n",
    "\n",
    "model_baseline.add(layers.Dropout(0.2, input_shape=(n_input,)))\n",
    "\n",
    "model_baseline.add(layer=layers.Dense(\n",
    "    units=1000,\n",
    "    activation='relu',\n",
    "    input_shape=(n_input,),\n",
    "    kernel_regularizer=regularizers.L2(.25)\n",
    "    ))\n",
    "\n",
    "model_baseline.add(layers.Dropout(0.2))\n",
    "\n",
    "model_baseline.add(layer=layers.Dense(\n",
    "    units=500,\n",
    "    activation='relu',\n",
    "    kernel_regularizer=regularizers.L2(.25)\n",
    "    ))\n",
    "\n",
    "model_baseline.add(layer=layers.Dense(\n",
    "    units=1,\n",
    "    ))\n",
    "\n",
    "model_baseline.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='mse',\n",
    "    metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "\n",
    "early_stopping = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10)\n",
    "    ]\n",
    "\n",
    "model_hist_baseline = model_baseline.fit(\n",
    "    np.array(X_train_vec_df_scaled),\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_vec_df_scaled, y_val),\n",
    "    verbose=True,\n",
    "    callbacks=early_stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nn_curves(model_history):\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18,5))\n",
    "    fl_ax = axes.flatten()\n",
    "    for idx, metric in enumerate(['loss', 'root_mean_squared_error']):\n",
    "        pair = [m for m in model_history.history.keys() if metric in m]\n",
    "        fl_ax[idx].plot(model_history.history[pair[0]], label=metric)\n",
    "        fl_ax[idx].plot(model_history.history[pair[1]], label=metric+'_val')\n",
    "        fl_ax[idx].set_xlabel('epochs')\n",
    "        fl_ax[idx].set_ylabel(metric)\n",
    "        fl_ax[idx].set_title(f'{metric.upper()} Evaluation')\n",
    "        fl_ax[idx].legend()\n",
    "        plt.tight_layout();\n",
    "\n",
    "plot_nn_curves(model_hist_baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_baseline.evaluate(X_val_vec_df_scaled, y_val, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val[17:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model_baseline.predict(X_val_vec_df_scaled))[17:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemmatize(text):\n",
    "    \"\"\"\n",
    "    Uses NLTK and SpaCy to tokenize a string and return the lemma of each token.\n",
    "    \"\"\"\n",
    "    sents = [s.text for s in nlp(text).sents]\n",
    "    sents_tokenized = [tokenizer.tokenize(sent) for sent in sents]\n",
    "    docs = [nlp(' '.join(tokens)) for tokens in sents_tokenized]\n",
    "    docs_lemmatized = [[t.lemma_.lower() for t in doc] for doc in docs]\n",
    "    return list(itertools.chain.from_iterable(docs_lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comments_lem'] = df['comments'].apply(spacy_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comments_lem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_lem_imploded = df.groupby('id').agg(\n",
    "    {'comments_lem': lambda x: list(itertools.chain.from_iterable(x))}\n",
    "    ).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_imploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_movies = df.drop(columns=['comments', 'comments_lem']).drop_duplicates(subset='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.merge(\n",
    "    left=just_movies,\n",
    "    right=comments_lem_imploded,\n",
    "    how='inner',\n",
    "    on='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['comments_lem_no_sw'] = df2['comments_lem'].apply(lambda x: [t for t in x if t not in stopword_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = r\"[a-zA-Z]+'?[a-zA-Z]+|\\b[iIaA]\\b\"\n",
    "# Pattern: Any word with at least two characters, including up to one apostrophe\n",
    "# Also captures the English words \"I\" and \"a\".\n",
    "\n",
    "tokenizer = RegexpTokenizer(token_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_spacy = list(nlp.Defaults.stop_words)\n",
    "sw_nltk = stopwords.words('english')\n",
    "stopword_list = list(set(sw_spacy + sw_nltk))\n",
    "stopword_list.extend([\"i'm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemmatize(text):\n",
    "    \"\"\"\n",
    "    Uses NLTK and SpaCy to tokenize a string and return the lemma of each token.\n",
    "    \"\"\"\n",
    "    sents = [s.text for s in nlp(text).sents]\n",
    "    sents_tokenized = [tokenizer.tokenize(sent) for sent in sents]\n",
    "    docs = [nlp(' '.join(tokens)) for tokens in sents_tokenized]\n",
    "    docs_lemmatized = [[t.lemma_.lower() for t in doc] for doc in docs]\n",
    "    return list(itertools.chain.from_iterable(docs_lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = df['comments'].loc[70690]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = r\"[a-zA-Z]+'?[a-zA-Z]+|\\b[iIaA]\\b\"\n",
    "# Pattern: Any word with at least two characters, including up to one apostrophe\n",
    "# Also captures the English words \"I\" and \"a\".\n",
    "\n",
    "tokenizer = RegexpTokenizer(token_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_spacy = list(nlp.Defaults.stop_words)\n",
    "sw_nltk = stopwords.words('english')\n",
    "stopword_list = list(set(sw_spacy + sw_nltk))\n",
    "stopword_list.extend([\"i'm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemmatize(text):\n",
    "    \"\"\"\n",
    "    Uses NLTK and SpaCy to tokenize a string and return the lemma of each token.\n",
    "    \"\"\"\n",
    "    sents = [s.text for s in nlp(text).sents]\n",
    "    sents_tokenized = [tokenizer.tokenize(sent) for sent in sents]\n",
    "    docs = [nlp(' '.join(tokens)) for tokens in sents_tokenized]\n",
    "    docs_lemmatized = [[t.lemma_.lower() for t in doc] for doc in docs]\n",
    "    return list(itertools.chain.from_iterable(docs_lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(df.index)\n",
    "\n",
    "display(df.loc[[idx]])\n",
    "\n",
    "print(spacy_lemmatize(df.loc[idx]['comments']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('capstone-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "741bbb06d8fece82f103e7a4762f2cad41a4667d502102b48dc88a29a56aa95e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
