{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __TITLE TK__\n",
    "## Using Neural Networks to and Reddit Comments to Predict Movie Ratings\n",
    "\n",
    "Flatiron School Data Science: Capstone Project\n",
    "\n",
    "- **Author**: Zaid Shoorbajee\n",
    "- **Instructor**: Morgan Jones\n",
    "- **Pace**: Flex, 40 weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IMAGE TK TK TK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding <a name=\"biz\"></a>\n",
    "\n",
    "A movie studio, Cinedistance, wants to know how well its movies will be received by audiences before it releases them. After all the hard work of producing a big-budget movie, there are still many decisions about the marketing and release of the movie. How much should the studio invest in marketing? Should it release it to a streaming or theaters? To make these decisions, it's helpful to know how much audiences will actually like the movie first.\n",
    "\n",
    "Cinedistance wants to get qualitative comments from average moviegoers *before* a movie's official release. It is also employing a data scientist to use these comments to predict a rating for the movie, like an IMDb score. It's piloting a program where the movie is released to a focus group of about 100 average moviegoers. These viewers are asked to sign an NDA, watch the movie, and submit their thoughts about the movie as if they are commenting on an internet comment section.\n",
    "\n",
    "The task of the data scientist is to apply natural language processing (NLP) and machine learning (ML) to the focus group's comments in order to predict a movie's IMDb score. These qualitative comments and predicted score can inform Cinedistance's decision-making about the movie's marketing, release. The studio may decide to do re-shoots, re-edits, or even [kill the movie](https://variety.com/2022/film/news/batgirl-not-released-warner-bros-hbo-max-1235331897/).\n",
    "\n",
    "<!-- An international news outlet, *The Flatiron Post*, wants to be able to report on stories of crises and natural disasters in a prompt manner. News about plane crashes, hurricanes, earthquakes, terrorist threats, and other topics occurs without warning. Being late to the story can mean not only losing to the competition, but also leaving your audience in the dark while speculation runs amok. \n",
    "\n",
    "The *Post* wants to tap into Twitter as a resource in order to detect such disasters in real time, and it’s employing a data scientist for the task. Twitter is a fire hose of information; there is a lot more noise than signal, and reporters would waste a lot of time staring at their Twitter feeds just waiting for disaster tweets. But chances are that if a disaster is occurring, someone is tweeting about it. \n",
    "\n",
    "The task of the data scientist is to use natural language processing (NLP) and machine learning in order to systematically tell if a tweet is about a real disaster or not. Such tweets can then theoretically be presented to the newsroom in a separate feed. Reporters can then choose to pursue that story or not. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding <a name=\"data_understanding\"></a>\n",
    "\n",
    "### Dataset <a name=\"dataset\"></a>\n",
    "\n",
    "Movie fans do do the type of commentary that Cinedistance is looking for everyday in places like the [Reddit community r/movies](reddit.com/r/movies). On **r/movies**, Reddit users share news and opinions about movies. Additionally, for most major movies that come out, the subreddit hosts an official discussion of the movie. These official discussions contains text data that can be the basis to train an ML model to predict IMDb scores.\n",
    "\n",
    "Reddit has an API that allows developers to scrape such information. The [PRAW library](https://praw.readthedocs.io/) simplifies this, acting as a wrapper for the API. Using PRAW, I scraped from r/movies the **highest-voted 100 comments of as many official movie discussions still indexed on Reddit**. Some discussions had fewer than 100 comments. I also downloaded [freely available ratings  data from IMDb](https://www.imdb.com/interfaces/) and matched scores to r/movies discussions.\n",
    "\n",
    " The resulting dataset contains:\n",
    "* 922 movies\n",
    "* 70,693 comments\n",
    "* Movie title\n",
    "* Reddit post ID and IMDB ID\n",
    "* Reddit discussion date\n",
    "* IMDb average rating **(target variable)**\n",
    "* Number of IMDb votes\n",
    "* Movie runtimes\n",
    "* Genres\n",
    "\n",
    "### NLP <a name=\"nlp\"></a>\n",
    "\n",
    "The core type of data being used for this task is the text of Reddit comments. This is **unstructured data** and requires natural language processing (NLP) techniques in order to be interpretable by a machine learning model, such as a deep neural network. \n",
    "\n",
    "Working with natural language is messy; different comments can have many of the same words, but context changes everything. It's easy for people to discern the difference, but for a computer, it's not so simple. To make comments interpretable by a neural network, this project uses the following NLP techniques:\n",
    "\n",
    "* Tokenization\n",
    "* Lemmatization\n",
    "* Removing stop words\n",
    "* TF-IDF Vectorization\n",
    "* Part-of-speech tagging\n",
    "* Sentiment analysis\n",
    "* Meta-feature extraction\n",
    "\n",
    "The idea is that converting comments into the signals listed above should help a machine learning model to discern a relationship between the comments and IMDb scores using hidden patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "This project ultimately is a regression task. I will be using [TensorFlow through the Keras interface](https://www.tensorflow.org/api_docs/python/tf/keras) in order to build a deep neural network. The neural network will be trained on a preprocessed version of the r/movies dataset that I have built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring and Evaluation\n",
    "\n",
    "The business case that a movie studio wants to know the IMDb rating of a move before it is released. The IMDb scores the models are trained on have a degree of variability that can't be accounted for 100% of the time with predictions. The R-squared score tells us what percent of the variability of the target variable the model accounts for, so R-squared will be reported to the stakeholder.\n",
    "\n",
    "Models in this notebook will also report the mean squared error (MSE), which is a measure of average distance of predictions from the true target values. Minimizing MSE is what the models will be optimizing for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "def time_check(start=None):\n",
    "    if start:\n",
    "        t = time.time() - start\n",
    "    else:\n",
    "        t = time.time() - start_time\n",
    "    print(f'Time check: {t//60:.0f} minutes and {t%60:.0f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow_addons.metrics import RSquare\n",
    "\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "%matplotlib inline\n",
    "\n",
    "import praw\n",
    "\n",
    "import json\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.max_rows = 100\n",
    "seed = 55\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this notebook was compiled by me in several other notebooks. See the [compile_and_filter_dataset](./compile_and_filter_dataset/) folder in this repository for the detailed process of how the data was collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in final dataset, containing reddit comments and IMDb scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/data_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tconst</th>\n",
       "      <th>title</th>\n",
       "      <th>originalTitle</th>\n",
       "      <th>comments</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>startYear</th>\n",
       "      <th>post_date_utc</th>\n",
       "      <th>post_year</th>\n",
       "      <th>post_month</th>\n",
       "      <th>post_day</th>\n",
       "      <th>genres</th>\n",
       "      <th>numVotes</th>\n",
       "      <th>averageRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vzcwal</td>\n",
       "      <td>tt13406136</td>\n",
       "      <td>the princess</td>\n",
       "      <td>The Princess</td>\n",
       "      <td>Joey King needs a new agent. She’s proven she has talent but she has so many terrible films on h...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.657851e+09</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>Action,Drama,Fantasy</td>\n",
       "      <td>11474</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vzcwal</td>\n",
       "      <td>tt13406136</td>\n",
       "      <td>the princess</td>\n",
       "      <td>The Princess</td>\n",
       "      <td>Silly, but entertaining and non stop action</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.657851e+09</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>Action,Drama,Fantasy</td>\n",
       "      <td>11474</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vzcwal</td>\n",
       "      <td>tt13406136</td>\n",
       "      <td>the princess</td>\n",
       "      <td>The Princess</td>\n",
       "      <td>The yassification of The Raid\\n\\nActually, this was fun enough and mad respect to Joey King for ...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.657851e+09</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>Action,Drama,Fantasy</td>\n",
       "      <td>11474</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vzcwal</td>\n",
       "      <td>tt13406136</td>\n",
       "      <td>the princess</td>\n",
       "      <td>The Princess</td>\n",
       "      <td>Honestly, this was pretty fun.  The plot is nothing special yes.\\n\\nBut Joey King was actually e...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.657851e+09</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>Action,Drama,Fantasy</td>\n",
       "      <td>11474</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vzcwal</td>\n",
       "      <td>tt13406136</td>\n",
       "      <td>the princess</td>\n",
       "      <td>The Princess</td>\n",
       "      <td>Man, I loved this movie. Yeah, it was campy, but whatever. The premise worked for me, I liked th...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.657851e+09</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>Action,Drama,Fantasy</td>\n",
       "      <td>11474</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      tconst         title originalTitle  \\\n",
       "0  vzcwal  tt13406136  the princess  The Princess   \n",
       "1  vzcwal  tt13406136  the princess  The Princess   \n",
       "2  vzcwal  tt13406136  the princess  The Princess   \n",
       "3  vzcwal  tt13406136  the princess  The Princess   \n",
       "4  vzcwal  tt13406136  the princess  The Princess   \n",
       "\n",
       "                                                                                              comments  \\\n",
       "0  Joey King needs a new agent. She’s proven she has talent but she has so many terrible films on h...   \n",
       "1                                                          Silly, but entertaining and non stop action   \n",
       "2  The yassification of The Raid\\n\\nActually, this was fun enough and mad respect to Joey King for ...   \n",
       "3  Honestly, this was pretty fun.  The plot is nothing special yes.\\n\\nBut Joey King was actually e...   \n",
       "4  Man, I loved this movie. Yeah, it was campy, but whatever. The premise worked for me, I liked th...   \n",
       "\n",
       "   runtimeMinutes  startYear  post_date_utc  post_year  post_month  post_day  \\\n",
       "0            94.0       2022   1.657851e+09       2022           7        14   \n",
       "1            94.0       2022   1.657851e+09       2022           7        14   \n",
       "2            94.0       2022   1.657851e+09       2022           7        14   \n",
       "3            94.0       2022   1.657851e+09       2022           7        14   \n",
       "4            94.0       2022   1.657851e+09       2022           7        14   \n",
       "\n",
       "                 genres  numVotes  averageRating  \n",
       "0  Action,Drama,Fantasy     11474            5.6  \n",
       "1  Action,Drama,Fantasy     11474            5.6  \n",
       "2  Action,Drama,Fantasy     11474            5.6  \n",
       "3  Action,Drama,Fantasy     11474            5.6  \n",
       "4  Action,Drama,Fantasy     11474            5.6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70692 entries, 0 to 70691\n",
      "Data columns (total 14 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   id              70692 non-null  object \n",
      " 1   tconst          70692 non-null  object \n",
      " 2   title           70692 non-null  object \n",
      " 3   originalTitle   70692 non-null  object \n",
      " 4   comments        70692 non-null  object \n",
      " 5   runtimeMinutes  70692 non-null  float64\n",
      " 6   startYear       70692 non-null  int64  \n",
      " 7   post_date_utc   70692 non-null  float64\n",
      " 8   post_year       70692 non-null  int64  \n",
      " 9   post_month      70692 non-null  int64  \n",
      " 10  post_day        70692 non-null  int64  \n",
      " 11  genres          70621 non-null  object \n",
      " 12  numVotes        70692 non-null  int64  \n",
      " 13  averageRating   70692 non-null  float64\n",
      "dtypes: float64(3), int64(5), object(6)\n",
      "memory usage: 7.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Imploding\" the dataframe\n",
    "\n",
    "Currently, each comment is its own row, giving the dataset over 70,000 rows.\n",
    "\n",
    "In reality, each of these comments corresponds to one of about 900 movies.\n",
    "\n",
    "That's how I'm going to be dealing with this data from now own, so I will compress all the comments into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_imploded = df.groupby('id')['comments'].agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tconst</th>\n",
       "      <th>title</th>\n",
       "      <th>originalTitle</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>startYear</th>\n",
       "      <th>post_date_utc</th>\n",
       "      <th>post_year</th>\n",
       "      <th>post_month</th>\n",
       "      <th>post_day</th>\n",
       "      <th>genres</th>\n",
       "      <th>numVotes</th>\n",
       "      <th>averageRating</th>\n",
       "      <th>comments</th>\n",
       "      <th>n_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vzcwal</td>\n",
       "      <td>tt13406136</td>\n",
       "      <td>the princess</td>\n",
       "      <td>The Princess</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.657851e+09</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>Action,Drama,Fantasy</td>\n",
       "      <td>11474</td>\n",
       "      <td>5.6</td>\n",
       "      <td>[Joey King needs a new agent. She’s proven she has talent but she has so many terrible films on ...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>vzcw0a</td>\n",
       "      <td>tt11671006</td>\n",
       "      <td>the man from toronto</td>\n",
       "      <td>The Man from Toronto</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.657851e+09</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>Action,Adventure,Comedy</td>\n",
       "      <td>43386</td>\n",
       "      <td>5.8</td>\n",
       "      <td>[ O offence to Woody but I feel like the original casting of Jason Statham would have at least i...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>vzcvsd</td>\n",
       "      <td>tt9288046</td>\n",
       "      <td>the sea beast</td>\n",
       "      <td>The Sea Beast</td>\n",
       "      <td>115.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.657851e+09</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>Adventure,Animation,Comedy</td>\n",
       "      <td>35834</td>\n",
       "      <td>7.1</td>\n",
       "      <td>[Absolutely crazy that Netflix dropped this and also The Mitchells Vs The Machines with almost n...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>vzcvkz</td>\n",
       "      <td>tt5151570</td>\n",
       "      <td>mrs harris goes to paris</td>\n",
       "      <td>Mrs. Harris Goes to Paris</td>\n",
       "      <td>115.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.657851e+09</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>Comedy,Drama</td>\n",
       "      <td>4798</td>\n",
       "      <td>7.1</td>\n",
       "      <td>[This was so cute it just made me smile the whole time.  Highly recommend., The only word for th...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>vzcv66</td>\n",
       "      <td>tt9411972</td>\n",
       "      <td>where the crawdads sing</td>\n",
       "      <td>Where the Crawdads Sing</td>\n",
       "      <td>125.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.657851e+09</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>Drama,Mystery,Thriller</td>\n",
       "      <td>28694</td>\n",
       "      <td>7.1</td>\n",
       "      <td>[I did enjoy her house representing the 2 different ways the men treated her . Tate was invited ...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id      tconst                     title              originalTitle  \\\n",
       "0    vzcwal  tt13406136              the princess               The Princess   \n",
       "21   vzcw0a  tt11671006      the man from toronto       The Man from Toronto   \n",
       "44   vzcvsd   tt9288046             the sea beast              The Sea Beast   \n",
       "121  vzcvkz   tt5151570  mrs harris goes to paris  Mrs. Harris Goes to Paris   \n",
       "143  vzcv66   tt9411972   where the crawdads sing    Where the Crawdads Sing   \n",
       "\n",
       "     runtimeMinutes  startYear  post_date_utc  post_year  post_month  \\\n",
       "0              94.0       2022   1.657851e+09       2022           7   \n",
       "21            110.0       2022   1.657851e+09       2022           7   \n",
       "44            115.0       2022   1.657851e+09       2022           7   \n",
       "121           115.0       2022   1.657851e+09       2022           7   \n",
       "143           125.0       2022   1.657851e+09       2022           7   \n",
       "\n",
       "     post_day                      genres  numVotes  averageRating  \\\n",
       "0          14        Action,Drama,Fantasy     11474            5.6   \n",
       "21         14     Action,Adventure,Comedy     43386            5.8   \n",
       "44         14  Adventure,Animation,Comedy     35834            7.1   \n",
       "121        14                Comedy,Drama      4798            7.1   \n",
       "143        14      Drama,Mystery,Thriller     28694            7.1   \n",
       "\n",
       "                                                                                                comments  \\\n",
       "0    [Joey King needs a new agent. She’s proven she has talent but she has so many terrible films on ...   \n",
       "21   [ O offence to Woody but I feel like the original casting of Jason Statham would have at least i...   \n",
       "44   [Absolutely crazy that Netflix dropped this and also The Mitchells Vs The Machines with almost n...   \n",
       "121  [This was so cute it just made me smile the whole time.  Highly recommend., The only word for th...   \n",
       "143  [I did enjoy her house representing the 2 different ways the men treated her . Tate was invited ...   \n",
       "\n",
       "     n_comments  \n",
       "0            21  \n",
       "21           23  \n",
       "44           77  \n",
       "121          22  \n",
       "143          93  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset='id').drop(columns=['comments'])\n",
    "df = df.join(comments_imploded, on='id')\n",
    "df['n_comments'] = df['comments'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the dataset to only *potentially* relevant features, although I may not use all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\n",
    "    'id', 'tconst', 'title', 'averageRating', 'numVotes',\n",
    "    'runtimeMinutes', 'genres', 'comments', 'n_comments'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                0\n",
       "tconst            0\n",
       "title             0\n",
       "averageRating     0\n",
       "numVotes          0\n",
       "runtimeMinutes    0\n",
       "genres            1\n",
       "comments          0\n",
       "n_comments        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only feature with a missing values is `genre`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tconst</th>\n",
       "      <th>title</th>\n",
       "      <th>averageRating</th>\n",
       "      <th>numVotes</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>genres</th>\n",
       "      <th>comments</th>\n",
       "      <th>n_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>t0tapr</td>\n",
       "      <td>tt15374070</td>\n",
       "      <td>studio 666</td>\n",
       "      <td>5.7</td>\n",
       "      <td>7356</td>\n",
       "      <td>106.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Scooby Foo, As “okay” as the film is, the chainsaw double kill was absolutely gnarly., There’s ...</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      tconst       title  averageRating  numVotes  runtimeMinutes  \\\n",
       "2378  t0tapr  tt15374070  studio 666            5.7      7356           106.0   \n",
       "\n",
       "     genres  \\\n",
       "2378    NaN   \n",
       "\n",
       "                                                                                                 comments  \\\n",
       "2378  [Scooby Foo, As “okay” as the film is, the chainsaw double kill was absolutely gnarly., There’s ...   \n",
       "\n",
       "      n_comments  \n",
       "2378          71  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['genres'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure why this wasn't in the dataset, but on IMDb's website, this movie's genres are comedy, horror, music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2378, 'genres'] = \"Comedy,Horror,Music\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine main dataset with sentiment dataset\n",
    "\n",
    "The sentiment dataframe was a last minute add-on in order to have feature-engineered sentiment labels for the Reddit comments I'm using. \n",
    "\n",
    "My documentation of how I got these scores is [here](\"./compile_and_filter_dataset/5_get_sentiments.ipynb). That data was collected in a separate notebook that was run on Google Colab for its cloud GPU capability.\n",
    "\n",
    "I used the Huggingface Transformers, along with a commonly used NLP model in order to get, for each movie, the proportion of positive, negative, and neutral comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>neg_norm</th>\n",
       "      <th>ntrl_norm</th>\n",
       "      <th>pos_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vzcwal</td>\n",
       "      <td>the princess</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vzcw0a</td>\n",
       "      <td>the man from toronto</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.227273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vzcvsd</td>\n",
       "      <td>the sea beast</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vzcvkz</td>\n",
       "      <td>mrs harris goes to paris</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.590909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vzcv66</td>\n",
       "      <td>where the crawdads sing</td>\n",
       "      <td>0.306818</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.465909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                     title  neg_norm  ntrl_norm  pos_norm\n",
       "0  vzcwal              the princess  0.238095   0.095238  0.666667\n",
       "1  vzcw0a      the man from toronto  0.409091   0.363636  0.227273\n",
       "2  vzcvsd             the sea beast  0.240000   0.120000  0.640000\n",
       "3  vzcvkz  mrs harris goes to paris  0.181818   0.227273  0.590909\n",
       "4  vzcv66   where the crawdads sing  0.306818   0.227273  0.465909"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments = pd.read_csv(\"./data/reddit_movie_sentiments.csv\")\n",
    "sentiments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_df = df.index\n",
    "\n",
    "df = pd.merge(\n",
    "    left=df,\n",
    "    right=sentiments.drop(columns=['title', 'ntrl_norm']),\n",
    "    how='inner',\n",
    "    on='id'\n",
    ")\n",
    "\n",
    "df.index = index_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the distribution of the target variable `averageRating`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "df['averageRating'].hist(ax=ax, color='#f14848', bins=20, grid=False)\n",
    "ax.set_title('Distribution of IMDb Ratings in dataset\\n')\n",
    "ax.set_xlabel('IMDb Score', size=12),\n",
    "ax.set_ylabel('Frequency of Score', size=12)\n",
    "fig.set_facecolor('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['averageRating'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(columns=['averageRating'])\n",
    "target = df['averageRating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train: Used for training the model and extracting meta-data about comments\n",
    "\n",
    "X_val: Used for selecting a model\n",
    "\n",
    "X_test: Used to score the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the full dataset into training and testing data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "# Splitting off a validation set\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=.5, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and tokenizing comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comments(comments_list):\n",
    "    \n",
    "    # Make everything lowercase\n",
    "    cleaned = [comment.lower() for comment in comments_list]\n",
    "\n",
    "    # Replace URLs with \"HYPERLINK\"\n",
    "    cleaned = [re.sub(pattern=r'http\\S+', repl='HYPERLINK', string=comment) for comment in cleaned]\n",
    "\n",
    "    # Remove excessive white space and newlines\n",
    "    cleaned = [comment.replace(\"\\n\", \" \") for comment in cleaned]\n",
    "    cleaned = [re.sub(pattern=r' {2,}', repl=' ', string=comment) for comment in cleaned]\n",
    "\n",
    "    # Ensure apostrophes and quotation marks are consistent\n",
    "    cleaned = [re.sub(r\"’|‘\", repl=\"'\", string=comment) for comment in cleaned]\n",
    "    cleaned = [re.sub(r\"“|”\", repl='\"', string=comment) for comment in cleaned]\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['comments'] = X_train['comments'].apply(clean_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview \n",
    "\n",
    "X_train.sample(5)['comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why not remove punctuation?\n",
    "\n",
    "Removing punctuation is a common step in cleaning text for NLP. I'm choosing not to yet since I plan to lemmatize words and extract part of speech tags. Doing so requires keeping grammatical context in the text, which would be lost if I removed punctuation at this point. The NLP library SpaCy treats punctuation as tokens, so I can simply treat punctuation as stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up a tokenizer with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabling certain components to keep it light\n",
    "\n",
    "nlp = spacy.load(\n",
    "    \"en_core_web_sm\",\n",
    "    disable=[\n",
    "        \"ner\", \"textcat\", \"custom\",\n",
    "        \"entity_linker\", \"entity_linker\",\n",
    "        \"textcat_multilabel\", \"transformer\"\n",
    "    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[3957]['comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of comments to use an example\n",
    "example_comments = X_train.loc[3957]['comments']\n",
    "display(example_comments[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How Spacy's \"nlp\" object works:\n",
    "\n",
    "print(\"Spacy Object:\")\n",
    "print(nlp.pipe(example_comments))\n",
    "print()\n",
    "print(\"The result is a list of docs. Each doc is a comment:\")\n",
    "print(list(nlp.pipe(example_comments))[0])\n",
    "print(list(nlp.pipe(example_comments))[1])\n",
    "print(list(nlp.pipe(example_comments))[2])\n",
    "print()\n",
    "print(\"Type:\")\n",
    "print(type((list(nlp.pipe(example_comments)))[0]))\n",
    "print()\n",
    "print(\"Tokens:\")\n",
    "print([t.text for t in list(nlp.pipe(example_comments))[0]])\n",
    "print([t.text for t in list(nlp.pipe(example_comments))[1]])\n",
    "print([t.text for t in list(nlp.pipe(example_comments))[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizing process appears to work. It treats most punctuation as tokens, so I will likely add punctuation to my stop word list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New column: Converting each reddit discussion into a list of SpaCy docs. Each doc is a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell can take over 7 minutes to run\n",
    "\n",
    "X_train['spacy_comments'] = X_train['comments'].apply(lambda x: list(nlp.pipe(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing\n",
    "\n",
    "#### I plan to make multiple tokenized versions of each comment.\n",
    "\n",
    "* Basic version: Any word with at least two letters. Strips the symbols for hashtags (#) and mentions(@)\n",
    "* Basic version, excluding stop words\n",
    "* Lemmatized version of basic version\n",
    "* Lemmatized version of basic version, exluding stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenize(list_of_docs):\n",
    "    \"\"\"\n",
    "    Takes in a list of SpaCy documents.\n",
    "    Converts them into one giant list of tokens as strings.    \n",
    "    \"\"\"\n",
    "    tokens_by_doc = [[t.text for t in doc] for doc in list_of_docs]\n",
    "    tokens_all_together = list(itertools.chain.from_iterable(tokens_by_doc))\n",
    "    return tokens_all_together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['tokens'] = X_train['spacy_comments'].apply(spacy_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words\n",
    "\n",
    "To make a comprehensive list of stop words, I will combine the default lists from the NLTK and SpaCy libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_spacy = list(nlp.Defaults.stop_words)\n",
    "sw_nltk = stopwords.words('english')\n",
    "punct = [p for p in string.punctuation]\n",
    "stopword_list = list(set(sw_spacy + sw_nltk + punct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any other other words I should include in stop words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "top_50_tok = OrderedDict(\n",
    "    FreqDist(X_train['tokens'].explode()).most_common(50)\n",
    "    )\n",
    "\n",
    "tokens = list(top_50_tok.keys())\n",
    "freq = list(top_50_tok.values())\n",
    "extra_sw = [t for t in tokens if t not in stopword_list]\n",
    "ax.bar(x=tokens, height=freq, color=['#f14848' if t in extra_sw else '#2c2fbf' for t in tokens])\n",
    "ax.set_ylabel('Frequency', size=10)\n",
    "ax.set_xlabel('Tokens', size=10)\n",
    "ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "ax.set_title('Top 50 tokens')\n",
    "custom_bars = [Line2D([0], [0], color='#f14848', lw=10), Line2D([0], [0], color='#2c2fbf', lw=10)]\n",
    "ax.legend(custom_bars, ['Not in stop words', 'In stop words'], fontsize=10)\n",
    "fig.set_facecolor('white');\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'./images/top_50_tokens', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokens **movie**, **like**, and **film** occur as frequently as many stop words. I'll add them to the stop word list. These words get thrown around so often in r/movies that they probably don't contribute much value in an ML context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list.extend([\"movie\", \"like\", \"film\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making tokens without stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['tokens_no_sw'] = X_train['tokens'].apply(\n",
    "    lambda x: [t for t in x if t not in stopword_list]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing most frequent tokens without stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,6))\n",
    "\n",
    "top_20_tok = OrderedDict(\n",
    "    FreqDist(X_train['tokens_no_sw'].explode()).most_common(20)\n",
    "    )\n",
    "\n",
    "tokens = list(top_20_tok.keys())[::-1]\n",
    "freq = list(top_20_tok.values())[::-1]\n",
    "ax.barh(y=tokens, width=freq, color='#f14848')\n",
    "ax.set_ylabel('Tokens', size=10)\n",
    "ax.set_xlabel('Frequency', size=10)\n",
    "ax.set_title('Top 20 tokens (no stop words)')\n",
    "fig.set_facecolor('white');\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'./images/top_20_tokens_no_sw', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting that commenters used elipses (\"...\") often enough that it's in the top 20 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previewing the restulting features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further breaking down tokens\n",
    "#### Lemmatization\n",
    "Now I want to make a version of these tokenized comments where each word is lemmatized. __Lemmatization__ is a technique that uses grammatical context to convert a word into its root word, or \"lemma,\" which is often its noun form. For example, the tool would change the words \"running,\" \"ran,\" and \"runs\" into \"run.\"\n",
    "\n",
    "Essentially I am another tokenizing function, where the output is lemmatized tokens instead of just plain tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemmatize(list_of_docs):\n",
    "    \"\"\"\n",
    "    Takes in a list of SpaCy documents.\n",
    "    Converts them into one giant list of lemmas as strings.    \n",
    "    \"\"\"\n",
    "    lemmas_by_doc = [[t.lemma_.lower() for t in doc] for doc in list_of_docs]\n",
    "    lemmas_all_together = list(itertools.chain.from_iterable(lemmas_by_doc))\n",
    "    return lemmas_all_together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['lemmas'] = \\\n",
    "    X_train['spacy_comments'].apply(spacy_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check most common lemmas\n",
    "\n",
    "FreqDist(X_train['lemmas'].explode()).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatized comments without stop words\n",
    "In order to make the lemmatized tokens without stop words, I will also need to lemmatize the stop words.\n",
    "\n",
    "This has to be done to the stop words while they are still within the string because SpaCy uses grammatical context to lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list_lem = []\n",
    "\n",
    "sw_lem_Series = \\\n",
    "    X_train['spacy_comments'].apply(\n",
    "        lambda x: [[t.lemma_.lower() for t in doc if t.text.lower() in stopword_list] for doc in x]\n",
    "    )\n",
    "\n",
    "sw_lem_Series = sw_lem_Series.apply(lambda x: list(itertools.chain.from_iterable(x)))\n",
    "\n",
    "for row in sw_lem_Series:\n",
    "    stopword_list_lem.extend(row)\n",
    "\n",
    "stopword_list_lem = list(set(stopword_list_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatized tokens, excluding stop words\n",
    "\n",
    "X_train['lemmas_no_sw'] = X_train['lemmas'].apply(\n",
    "    lambda x: [l for l in x if l not in stopword_list_lem]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seeing most frequent lemmas without stopwords:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,6))\n",
    "\n",
    "top_20_lem = OrderedDict(\n",
    "    FreqDist(X_train['lemmas_no_sw'].explode()).most_common(20)\n",
    "    )\n",
    "\n",
    "tokens = list(top_20_lem.keys())[::-1]\n",
    "freq = list(top_20_lem.values())[::-1]\n",
    "ax.barh(y=tokens, width=freq, color='#f14848')\n",
    "ax.set_ylabel('Lemmas', size=10)\n",
    "ax.set_xlabel('Frequency', size=10)\n",
    "ax.set_title('Top 20 lemmas (no stop words)')\n",
    "fig.set_facecolor('white');\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'./images/top_20_lemmas_no_sw', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing frequency distribution of tokens and lemmas for higher and lower IMDb scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean of y_train: {np.round(y_train.mean(), 2)}\")\n",
    "print(f\"Median of y_train: {np.round(y_train.median(), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificially giving y_train two classes (for visualization purposes).\n",
    "\n",
    "1: Above average\n",
    "\n",
    "0: Below average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_class = y_train.apply(lambda x: int(x > y_train.mean()))\n",
    "y_train_class.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_freqdict_classes(series, y, token_type='Tokens (unspecified kind))', cutoff=20):\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "    fd_0 = FreqDist(series[y==0].explode()).most_common(cutoff)\n",
    "    fd_0 = OrderedDict(fd_0)\n",
    "    tokens_0 = list(fd_0.keys())[::-1]\n",
    "    freq_0 = list(fd_0.values())[::-1]\n",
    "    fd_1 = FreqDist(series[y==1].explode()).most_common(cutoff)\n",
    "    fd_1 = OrderedDict(fd_1)\n",
    "    tokens_1 = list(fd_1.keys())[::-1]\n",
    "    freq_1 = list(fd_1.values())[::-1]\n",
    "    shared_tokens = [t for t in tokens_0 if t in tokens_1]\n",
    "    axes[0].barh(y=tokens_0, width=freq_0, color=['C6' if token in shared_tokens else 'C0' for token in tokens_0])\n",
    "    axes[1].barh(y=tokens_1, width=freq_1, color=['C6' if token in shared_tokens else 'C0' for token in tokens_1])\n",
    "    axes[0].set_ylabel('Tokens', size=10)\n",
    "    axes[0].set_xlabel('Frequency', size=10)\n",
    "    axes[1].set_xlabel('Frequency', size=10)\n",
    "    fig.suptitle(f'Top {cutoff} {token_type}', size=15)\n",
    "    axes[0].set_title('Below average IMDb Score')\n",
    "    axes[1].set_title('Above average average IMDb Score')\n",
    "    custom_bars = [Line2D([0], [0], color='C6', lw=10), Line2D([0], [0], color='C0', lw=10)]\n",
    "    axes[0].legend(custom_bars, ['In common', 'Not in common'])\n",
    "    axes[1].legend(custom_bars, ['In common', 'Not in common'])\n",
    "    fig.set_facecolor('white');\n",
    "    plt.tight_layout()\n",
    "#     plt.savefig(f'./images/top_20_{series.name}', dpi=500)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_freqdict_classes(X_train['tokens'], y_train_class, 'tokens')\n",
    "plot_freqdict_classes(X_train['tokens_no_sw'], y_train_class, 'tokens (no stop words)')\n",
    "plot_freqdict_classes(X_train['lemmas'], y_train_class, 'lemmas')\n",
    "plot_freqdict_classes(X_train['lemmas_no_sw'], y_train_class, 'lemmas (no stop words)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More linguistic feature engineering\n",
    "I will use the SpaCy library to extract more linguistic features from the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized part of speech (POS) tags\n",
    "The SpaCy library is pre-trained to parse through sentences and identify each word's grammatical part of speech.\n",
    "\n",
    "Here are some examples of what the tool can identify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_displacy_pos = {'compact':True, 'distance':90, 'bg':'#3056ff', 'color':'fff'}\n",
    "\n",
    "for idx in [2, 5, 12]:\n",
    "    print(example_comments[idx])\n",
    "    displacy.render(nlp(example_comments[idx]), style='dep', jupyter=True, options=options_displacy_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I plan to use this SpaCy capability in order to vectorize the universal [parts of speech](https://universaldependencies.org/u/pos/all.html) of each comment.\n",
    "\n",
    "First, I'll convert each token into a string of its POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_pos(list_of_docs):\n",
    "    \"\"\"\n",
    "    Takes in a list of SpaCy documents.\n",
    "    Converts them into one giant list of POS tags as strings.    \n",
    "    \"\"\"\n",
    "    pos_by_doc = [[t.pos_ for t in doc] for doc in list_of_docs]\n",
    "    pos_all_together = list(itertools.chain.from_iterable(pos_by_doc))\n",
    "    return pos_all_together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['spacy_pos'] = X_train['spacy_comments'].apply(spacy_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the POS vectors, I'm using scikit-learn's CountVectorizer in a slightly unorthodox way. I'm using it to count POS tags rather than tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataframes of vectorized POS tags\n",
    "\n",
    "pos_vectorizer = CountVectorizer(tokenizer=spacy_pos, lowercase=False)\n",
    "pos_vec_train = pos_vectorizer.fit_transform(X_train['spacy_comments'])\n",
    "pos_vec_df_train = pd.DataFrame(\n",
    "        pos_vec_train.toarray(),\n",
    "        columns=pos_vectorizer.get_feature_names_out(),\n",
    "        index=X_train.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vec_df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing this to the proportion of each POS within each Reddit thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vec_df_norm_train = pos_vec_df_train.div(pos_vec_df_train.sum(axis=1), axis=0)\n",
    "pos_vec_df_norm_train.columns = pos_vec_df_train.columns + '_norm'\n",
    "pos_vec_df_norm_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for linear relationship between POS tags and IMDb score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pearson_r(X, y=y_train):\n",
    "    return round(pearsonr(X, y_train)[0], 4)\n",
    "\n",
    "def get_p(X, y=y_train):\n",
    "    return pearsonr(X, y_train)[1]\n",
    "    \n",
    "def is_p_significant(X, y=y_train):\n",
    "    return pearsonr(X, y_train)[1] < 0.05\n",
    "\n",
    "POS_correlations =pd.DataFrame(\n",
    "    pos_vec_df_norm_train.agg(\n",
    "        [get_pearson_r, get_p, is_p_significant]\n",
    "        )).T\n",
    "\n",
    "POS_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty much all these relationships are weak, but I will still include at least the ones that have a significant relationship at a p-value of 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somewhat_significant_POS = \\\n",
    "    POS_correlations.query(\"is_p_significant == True\").index.tolist()\n",
    "somewhat_significant_POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vec_df_norm_train = pos_vec_df_norm_train[somewhat_significant_POS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-feature Engineering\n",
    "To engineer more features, I'm going to use seemingly arbitrary information from each comment section. Here's the set of meta-features I plan to make:\n",
    "\n",
    "* Number of comments per discussion\n",
    "* Average character count of comment per discussion\n",
    "* Average lemma length per discussion excluding stop words.\n",
    "* Proportion of unique lemmas excluding stop words.\n",
    "\n",
    "I drew inspiration for some of these features from [this Kaggle entry](https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert/notebook) by Gunes Evitan.\n",
    "\n",
    "Making a new DataFrame composed of the meta-features I listed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features_df_train = X_train[['n_comments']].copy()\n",
    "\n",
    "# Average character count of comment per discussion\n",
    "meta_features_df_train['avg_char_count_comments'] = \\\n",
    "    X_train['comments'].apply(lambda x: np.mean([len(comment) for comment in x]))\n",
    "\n",
    "# Average lemma length per discussion excluding stop words.\n",
    "meta_features_df_train['mean_lemma_length'] = \\\n",
    "    X_train['lemmas_no_sw'].apply(lambda x: np.mean([len(l) for l in x]))\n",
    "\n",
    "# Proportion of unique lemmas excluding stop words.\n",
    "meta_features_df_train['prop_unique_lemmas_no_sw'] = \\\n",
    "    X_train['lemmas_no_sw'].apply(lambda x: len(set(x))/len(x))\n",
    "\n",
    "meta_features_df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for linear relationship between POS tags and IMDb score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    meta_features_df_train.agg(\n",
    "        [get_pearson_r, get_p, is_p_significant]\n",
    "        )).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, these are pretty weak relationships, but in this case they're at least all significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can visualize density plots for these meta-features, separated by the artificial classes I created earlier (below or above average IMDb score). The features `mean_lemma_length` and `prop_unique_lemmas_no_sw` appear to have a difference in distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_histplot = {'kde':True, 'stat':\"density\", 'linewidth':0, 'bins':20}\n",
    "color_below = '#2c2fbf'\n",
    "color_above ='#f14848'\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,5))\n",
    "fl_ax = axes.flatten()\n",
    "fig.set_facecolor('white')\n",
    "for idx, ftr in list(enumerate(meta_features_df_train.columns)):\n",
    "    sns.histplot(meta_features_df_train[ftr][y_train_class==0], ax=fl_ax[idx], **kwargs_histplot, color=color_below)\n",
    "    sns.histplot(meta_features_df_train[ftr][y_train_class==1], ax=fl_ax[idx], **kwargs_histplot, color=color_above)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./images/meta_features_basic.png', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the comment sections\n",
    "\n",
    "I've engineered linguistic features and meta-features. Now I'm going to make the actual text of the comments interpretable by a machine learning model using [scikit-learn's TF-IDF vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). This vectorizer returns the [**term frequency-inverse document frequency**](https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089) (TF-IDF) of each token in each movie comment section.\n",
    "\n",
    "TF-IDF is a measurement that takes into account the 1) token's frequency within the document (term frequency) and 2) how rare it is for the token to appear in a document (inverse document frequency). \n",
    "\n",
    "I'm using the lemmatized version of comment sections with stop words removed.\n",
    "<!-- Recall that this is the version that looked the most different between classes when I graphed token frequencies earlier. The vectorizer will return TF-IDF values for the top 500 frequently occurring lemmas across the \"corpus\" -- which is the collection of comment sections. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer=spacy_lemmatize, \n",
    "    stop_words=stopword_list_lem,\n",
    "    max_features=500,\n",
    "    lowercase=False\n",
    "    )\n",
    "\n",
    "X_train_vec = tfidf.fit_transform(X_train['spacy_comments'])\n",
    "\n",
    "X_train_vec_df = pd.DataFrame(\n",
    "    X_train_vec.toarray(),\n",
    "    columns=tfidf.get_feature_names_out(),\n",
    "    index=X_train.index\n",
    "    )\n",
    "X_train_vec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I'm going to combine the vectors with the other features I've engineered so far into a single DataFrame. All features are being scaled, as well.\n",
    "\n",
    "**This is the DataFrame that the model will train on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined_df = pd.concat(\n",
    "    [\n",
    "        X_train_vec_df,\n",
    "        pos_vec_df_norm_train, \n",
    "        meta_features_df_train,\n",
    "    ],\n",
    "    axis=1\n",
    "    )\n",
    "\n",
    "# Scaling all features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_combined_df_scaled = scaler.fit_transform(X_train_combined_df)\n",
    "X_train_combined_df_scaled = pd.DataFrame(X_train_combined_df_scaled, index=X_train.index, columns=X_train_combined_df.columns)\n",
    "\n",
    "X_train_combined_df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply all pre-processing steps to test and validation sets.\n",
    "The function below runs the `test` and `val` sets through the exact same preprocessing steps that the `train` set as undergone.\n",
    "\n",
    "By default, the function makes use of the exact transformer objects that have been trained on `X_train`, in order to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_comments(\n",
    "    df_to_process,\n",
    "    tokenizer=spacy_tokenize,\n",
    "    stop_words=stopword_list,\n",
    "    lem_tokenizer=spacy_lemmatize,\n",
    "    stop_words_lem=stopword_list_lem,\n",
    "    trained_pos_vectorizer=pos_vectorizer,\n",
    "    trained_word_vectorizer=tfidf,\n",
    "    trained_scaler=scaler,\n",
    "    return_scaled=True,\n",
    "    return_cleaned_df=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe with a column named \"comments\", where each\n",
    "    row contains a list of individual Reddit comments about a movie.\n",
    "    All other parameters have default arguments set to specific\n",
    "    tokenizers, vectorizers, and stop word lists. By default, this \n",
    "    function makes use of transformer objects that have been trained \n",
    "    on X_train in this notebook, in order to avoid data leakage. If \n",
    "    you are reusing this function in a different project, you should\n",
    "    rewrite the function with your default arguments.\n",
    "\n",
    "    SUMMARY OF ACTIONS:\n",
    "\n",
    "    - Clean comments -- Remove URLs, make everything lowercase,\n",
    "        standardize quotation marks and apostrophes, remove extra\n",
    "        whitespace and newlines.\n",
    "    - Convert lists of comments into lists of SpaCy documents.\n",
    "    - Use SpaCy documents to tokenize comments.\n",
    "    - Use SpaCy documents to lemmatize comoments.\n",
    "    - Use SpaCy documents and a fitted Sklearn CountVectorizer \n",
    "        object to count and normalize part-of-speech tags; \n",
    "        create POS dataframe.\n",
    "    - Calculate meta-features; create meta-features dataframe.\n",
    "    - Vectorize lemmas or tokens using a fitted Sklearn \n",
    "        TfidfVectorizer object (TF-IDF vectorization).\n",
    "    - Combine all features dataframes into one.\n",
    "    - Scale resulting dataframe using a fitted Sklearn \n",
    "        StandardScaler.\n",
    "    - If \"return_cleaned_df\" parameter set to True, returns\n",
    "        a dictionary of both the features dataframe and the\n",
    "        dataframe wit the cleaned and tokenized version of \n",
    "        the comments. Else, only returns features.\n",
    "    \"\"\"\n",
    "    comments_df = df_to_process.copy()\n",
    "    \n",
    "    comments_df['comments'] = comments_df['comments'].apply(clean_comments)\n",
    "    comments_df['spacy_comments'] = comments_df['comments'].apply(lambda x: list(nlp.pipe(x)))\n",
    "\n",
    "    comments_df['tokens'] = comments_df['spacy_comments'].apply(tokenizer)\n",
    "    comments_df['tokens_no_sw'] = \\\n",
    "        comments_df['tokens'].apply(lambda x: [t for t in x if t not in stop_words])\n",
    "\n",
    "    comments_df['lemmas'] = comments_df['spacy_comments'].apply(lem_tokenizer)\n",
    "    comments_df['lemmas_no_sw'] = \\\n",
    "        comments_df['lemmas'].apply(lambda x: [t for t in x if t not in stop_words_lem])\n",
    "\n",
    "    pos_vec = trained_pos_vectorizer.transform(comments_df['spacy_comments'])\n",
    "    pos_vec_df = pd.DataFrame(\n",
    "        pos_vec.toarray(), \n",
    "        columns=trained_pos_vectorizer.get_feature_names_out(),\n",
    "        index=df_to_process.index\n",
    "    )\n",
    "    pos_vec_df_norm = pos_vec_df.div(pos_vec_df.sum(axis=1), axis=0)\n",
    "    pos_vec_df_norm.columns = pos_vec_df.columns + '_norm'\n",
    "    pos_vec_df_norm = pos_vec_df_norm[somewhat_significant_POS]\n",
    "\n",
    "    meta_features_df = comments_df[['n_comments']].copy()\n",
    "    # Average character count of comment per discussion\n",
    "    meta_features_df['avg_char_count_comments'] = \\\n",
    "        comments_df['comments'].apply(lambda x: np.mean([len(comment) for comment in x]))\n",
    "    # Average lemma length per discussion. No stop words.\n",
    "    meta_features_df['mean_lemma_length'] = \\\n",
    "        comments_df['lemmas_no_sw'].apply(lambda x: np.mean([len(l) for l in x]))\n",
    "    # Proportion of unique lemmas. No stop words.\n",
    "    meta_features_df['prop_unique_lemmas_no_sw'] = \\\n",
    "        comments_df['lemmas_no_sw'].apply(lambda x: len(set(x))/len(x))\n",
    "    \n",
    "    comments_vec = trained_word_vectorizer.transform(comments_df['spacy_comments'])\n",
    "    comments_vec_df = pd.DataFrame(\n",
    "        comments_vec.toarray(), \n",
    "        columns=trained_word_vectorizer.get_feature_names_out(),\n",
    "        index=df_to_process.index\n",
    "    )\n",
    "    comments_combined_df = pd.concat(\n",
    "        [\n",
    "            comments_vec_df,\n",
    "            pos_vec_df_norm,\n",
    "            meta_features_df\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "    comments_combined_df_scaled = trained_scaler.transform(comments_combined_df)\n",
    "    comments_combined_df_scaled = pd.DataFrame(\n",
    "        comments_combined_df_scaled, \n",
    "        index=df_to_process.index, \n",
    "        columns=comments_combined_df.columns\n",
    "        )\n",
    "\n",
    "    if return_scaled:\n",
    "        if return_cleaned_df:\n",
    "            return {'processed':comments_combined_df_scaled, 'cleaned':comments_df}\n",
    "        else:\n",
    "            return comments_combined_df_scaled\n",
    "    else:\n",
    "        if return_cleaned_df:\n",
    "            return {'processed':comments_combined_df, 'cleaned':comments_df}\n",
    "        else:\n",
    "            return comments_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_combined_df_scaled = preprocess_comments(X_test)\n",
    "X_val_combined_df_scaled = preprocess_comments(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "In this section I'll begin using TensorFlow's Keras wrapper to make deep neural networks to predict movies' IMDb scores.\n",
    "\n",
    "This will be an iterative process where I'll tweak various parameters and hyperparameters while monitoring each model's optimization and performance. Models will be judged based on performance with the validation dataset (X_val). \n",
    "\n",
    "I will use the rectified linear unit (ReLU) activation function in the input and hidden layers in order to induce non-linearity. ReLU is commonly cited is one of the more advanced activation functions.\n",
    "\n",
    "I'm using the Adam algorithm to optimize each model, as it's [cited](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) as a more efficient alternative to stochastic gradient descent. The aim is to minimize the loss, which will be mean squared error. MSE measures the average of the square of the distance between predictions and true values. This is mainly to monitor for things like overfitting, underfitting, and convergence. Models will be configured to automatically stop when the loss on the validation set hasn't improved in 20 epochs.\n",
    "\n",
    "Ultimately, however the model with the best R<sup>2</sup> score will be selected, given that there aren't over or underfitting issues. This is not necessarily the model with the lowest loss. R<sup>2</sup> indicates the proportion of the variability in the dependent variable that can be explained by the model.\n",
    "\n",
    "After selecting a model, it will be given a final R<sup>2</sup> score using the test set (X_test).\n",
    "\n",
    "### Note: Regression, assumptions, inferences, and predictions\n",
    "\n",
    "Often, when predicting a continuous variable such as IMDb score, linear regression is used. This is a basic machine learning task that tries to find an optimal linear relationship between the predictors and the target. Linear regression stipulates several statistical assumptions such as independence of observations, homoscedasticity, no multicollinearity, and others. These assumptions are necessary when needing to make inferences about how individual predictors influence the target. However in a modeling context, there is less of a concern about making such inferences and [more of an emphasis on making accurate predictions](https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full). For that reason, I haven't checked for any of the assumptions for linear or non-linear regression. Considering how messy NLP is, I highly doubt this data would satisfy them. I am simply going to be interpreting and evaluation how well the models can predict a movie rating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "The baseline model is a simple, untunedd linear regression model with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lr = LinearRegression()\n",
    "baseline_lr.fit(X_train_combined_df_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline training loss (MSE):\")\n",
    "print(mean_squared_error(y_train, baseline_lr.predict(X_train_combined_df_scaled)))\n",
    "print()\n",
    "print(\"Baseline validation loss (MSE):\")\n",
    "print(mean_squared_error(y_val, baseline_lr.predict(X_val_combined_df_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline training performance (R-squared):\")\n",
    "print(baseline_lr.score(X_train_combined_df_scaled, y_train))\n",
    "print()\n",
    "print(\"Baseline validation performance (R-squared):\")\n",
    "print(baseline_lr.score(X_val_combined_df_scaled, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline model validation results:\n",
    "* Loss: 0.7155\n",
    "* R<sup>2</sup>: 0.2236\n",
    "* There is clear overfitting, and this is is a pretty low score; not much variance in the target is explained by the model.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that  visualizes neural network performance\n",
    "\n",
    "def plot_nn_curves(model_history):\n",
    "    \"\"\"\n",
    "    Takes in the model history object from a fitted TensorFlow model. \n",
    "    Uses attributes from the history object to plot the training and \n",
    "    validation performance of model, according to the chosen loss\n",
    "    functions and metrics.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9,4))\n",
    "    fl_ax = axes.flatten()\n",
    "    for idx, metric in enumerate(['loss', model_history.model.metrics_names[1]]):\n",
    "        pair = [m for m in model_history.history.keys() if metric in m]\n",
    "        fl_ax[idx].plot(model_history.history[pair[0]], label=metric)\n",
    "        fl_ax[idx].plot(model_history.history[pair[1]], label=metric+'_val')\n",
    "        fl_ax[idx].set_xlabel('epochs')\n",
    "        fl_ax[idx].set_ylabel(metric)\n",
    "        fl_ax[idx].set_title(f'{metric.upper()} Evaluation')\n",
    "        fl_ax[idx].legend()\n",
    "        plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that returns loss and performance\n",
    "\n",
    "def model_evaluation(\n",
    "    model,\n",
    "    pred_train=X_train_combined_df_scaled,\n",
    "    target_train=y_train, \n",
    "    pred_val=X_val_combined_df_scaled,\n",
    "    target_val=y_val\n",
    "    ):\n",
    "    train_preds = model.predict(pred_train, verbose=False)\n",
    "    val_preds = model.predict(pred_val, verbose=False)\n",
    "    print(f\"{model.name} Evaluation:\")\n",
    "    print(\"*************** Loss (MSE) ****************\")\n",
    "    print(f\"Training: {mean_squared_error(target_train, train_preds)}\")\n",
    "    print(f\"Validation: {mean_squared_error(target_val, val_preds)}\")\n",
    "    print(\"********* Performance (R-squared) *********\")\n",
    "    print(f\"Training: {r2_score(target_train, train_preds)}\")\n",
    "    print(f\"Validation: {r2_score(target_val, val_preds)}\")\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1\n",
    "Single layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = X_train_combined_df_scaled.shape[1]\n",
    "n_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "m1 = models.Sequential(name=\"Model_1\")\n",
    "\n",
    "m1.add(layer=layers.Dense(\n",
    "    units=n_input + 1,\n",
    "    activation='relu',\n",
    "    input_shape=(n_input,)\n",
    "))\n",
    "\n",
    "m1.add(layer=layers.Dense(\n",
    "    units=1,\n",
    "    activation='linear',\n",
    "    ))\n",
    "\n",
    "m1.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='mse',\n",
    "    metrics=[RSquare()]\n",
    "    )\n",
    "\n",
    "early_stopping = [\n",
    "    EarlyStopping(monitor='val_loss', patience=20)\n",
    "    ]\n",
    "\n",
    "m1_hist = m1.fit(\n",
    "    np.array(X_train_combined_df_scaled),\n",
    "    y_train,\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_combined_df_scaled, y_val),\n",
    "    verbose=False,\n",
    "    callbacks=early_stopping\n",
    ")\n",
    "\n",
    "plot_nn_curves(m1_hist)\n",
    "print(model_evaluation(m1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 validation results\n",
    "* Loss: 0.9726\n",
    "* R<sup>2</sup>: -0.0555\n",
    "* A negative R<sup>2</sup> is never a good sign. It means the model is wholly unsuited to predict the variable. It's essentially doing worse than a horizontal line, not to mention the baseline linear regression model.\n",
    "* Based on the training and validation loss, the model is extremely overfit.\n",
    "* Perhaps with how complex the nlp dataset is, this neural network wasn't complex enough to find a relationship between predictors and target.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2\n",
    "Changes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "m2 = models.Sequential(name=\"Model_2\")\n",
    "\n",
    "m2.add(layer=layers.Dense(\n",
    "    units=n_input + 1,\n",
    "    activation='relu',\n",
    "    input_shape=(n_input,)\n",
    "))\n",
    "\n",
    "m2.add(layer=layers.Dense(\n",
    "    units=n_input/2,\n",
    "    activation='relu',\n",
    "    ))\n",
    "\n",
    "m2.add(layer=layers.Dense(\n",
    "    units=1,\n",
    "    activation='linear',\n",
    "    ))\n",
    "\n",
    "m2.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='mse',\n",
    "    metrics=[RSquare()]\n",
    "    )\n",
    "\n",
    "early_stopping = [\n",
    "    EarlyStopping(monitor='val_loss', patience=20)\n",
    "    ]\n",
    "\n",
    "m2_hist = m2.fit(\n",
    "    np.array(X_train_combined_df_scaled),\n",
    "    y_train,\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_combined_df_scaled, y_val),\n",
    "    verbose=False,\n",
    "    callbacks=early_stopping\n",
    ")\n",
    "\n",
    "plot_nn_curves(m2_hist)\n",
    "print(model_evaluation(m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "m3 = models.Sequential(name=\"Model_3\")\n",
    "\n",
    "m3.add(layer=layers.Dense(\n",
    "    units=n_input + 1,\n",
    "    activation='gelu',\n",
    "    input_shape=(n_input,),\n",
    "))\n",
    "\n",
    "m3.add(layers.Dropout(0.2))\n",
    "\n",
    "m3.add(layer=layers.Dense(\n",
    "    units=n_input/2,\n",
    "    activation='elu',\n",
    "    ))\n",
    "\n",
    "m3.add(layers.Dropout(0.2))\n",
    "\n",
    "m3.add(layer=layers.Dense(\n",
    "    units=1,\n",
    "    activation='linear',\n",
    "    ))\n",
    "\n",
    "m3.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=[RSquare()]\n",
    "    )\n",
    "\n",
    "early_stopping = [\n",
    "    EarlyStopping(monitor='val_loss', patience=20)\n",
    "    ]\n",
    "\n",
    "m3_hist = m3.fit(\n",
    "    np.array(X_train_combined_df_scaled),\n",
    "    y_train,\n",
    "    epochs=150,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_combined_df_scaled, y_val),\n",
    "    verbose=False,\n",
    "    callbacks=early_stopping\n",
    ")\n",
    "\n",
    "plot_nn_curves(m3_hist)\n",
    "print(model_evaluation(m3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seed(seed)\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Dropout(0.2, input_shape=(n_input,)))\n",
    "\n",
    "# model.add(layer=layers.Dense(\n",
    "#     units=round(n_input/2),\n",
    "#     activation='relu',\n",
    "#     kernel_regularizer=regularizers.L2(0.05)\n",
    "#     # input_shape=(n_input,)\n",
    "# ))\n",
    "\n",
    "# model.add(layers.Dropout(0.2))\n",
    "\n",
    "# model.add(layer=layers.Dense(\n",
    "#     units=round(n_input/4),\n",
    "#     activation='relu',\n",
    "#     kernel_regularizer=regularizers.L2(0.05)\n",
    "# ))\n",
    "\n",
    "# model.add(layers.Dropout(0.2))\n",
    "\n",
    "# model.add(layer=layers.Dense(\n",
    "#     units=1,\n",
    "#     activation='linear',\n",
    "#     ))\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer='sgd',\n",
    "#     loss='mean_absolute_error',\n",
    "#     metrics=[RSquare()]\n",
    "#     )\n",
    "\n",
    "# early_stopping = [\n",
    "#     EarlyStopping(monitor='val_loss', patience=10)\n",
    "#     ]\n",
    "\n",
    "# model_hist = model.fit(\n",
    "#     np.array(X_train_combined_df_scaled),\n",
    "#     y_train,\n",
    "#     epochs=150,\n",
    "#     batch_size=32,\n",
    "#     validation_data=(X_val_combined_df_scaled, y_val),\n",
    "#     verbose=False,\n",
    "#     callbacks=early_stopping\n",
    "# )\n",
    "\n",
    "# plot_nn_curves(model_hist)\n",
    "# print('Evaluation:')\n",
    "# display(model.evaluate(X_val_combined_df_scaled, y_val, return_dict=True))\n",
    "# print('R2:')\n",
    "# display(r2_score(y_val, model.predict(X_val_combined_df_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seed(seed)\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Dropout(0.2, input_shape=(n_input,)))\n",
    "\n",
    "# model.add(layer=layers.Dense(\n",
    "#     units=round(n_input/2),\n",
    "#     activation='relu',\n",
    "#     kernel_regularizer=regularizers.L2()\n",
    "#     # input_shape=(n_input,)\n",
    "# ))\n",
    "\n",
    "# model.add(layers.Dropout(0.2))\n",
    "\n",
    "# model.add(layer=layers.Dense(\n",
    "#     units=round(n_input/4),\n",
    "#     activation='relu',\n",
    "#     kernel_regularizer=regularizers.L2()\n",
    "# ))\n",
    "\n",
    "# model.add(layers.Dropout(0.2))\n",
    "\n",
    "# model.add(layer=layers.Dense(\n",
    "#     units=round(n_input/8),\n",
    "#     activation='relu',\n",
    "#     kernel_regularizer=regularizers.L2()\n",
    "# ))\n",
    "\n",
    "# model.add(layers.Dropout(0.2))\n",
    "\n",
    "# model.add(layer=layers.Dense(\n",
    "#     units=1,\n",
    "#     activation='linear',\n",
    "#     ))\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer='sgd',\n",
    "#     loss='mean_absolute_error',\n",
    "#     metrics=[RSquare()]\n",
    "#     )\n",
    "\n",
    "# early_stopping = [\n",
    "#     EarlyStopping(monitor='val_loss', patience=10)\n",
    "#     ]\n",
    "\n",
    "# model_hist = model.fit(\n",
    "#     np.array(X_train_combined_df_scaled),\n",
    "#     y_train,\n",
    "#     epochs=150,\n",
    "#     batch_size=32,\n",
    "#     validation_data=(X_val_combined_df_scaled, y_val),\n",
    "#     verbose=False,\n",
    "#     callbacks=early_stopping\n",
    "# )\n",
    "\n",
    "# plot_nn_curves(model_hist)\n",
    "# print('Evaluation:')\n",
    "# display(model.evaluate(X_val_combined_df_scaled, y_val, return_dict=True))\n",
    "# print('R2:')\n",
    "# display(r2_score(y_val, model.predict(X_val_combined_df_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reg_qq_sced(y, X, add_constant=True, qq=True, sced=True):\n",
    "#     \"\"\"\n",
    "#     Display a the summary output of a linear regression model, with predictors X and target y.\n",
    "\n",
    "#     Also displays a QQ plot and residual plot by default. These can be toggled off.\n",
    "    \n",
    "#     The function will add a constant to the predictors by default, and this can be toggled off.\n",
    "#     \"\"\"\n",
    "#     # Run a linear regression and display the summary\n",
    "#     if add_constant:\n",
    "#         X_sm = sm.add_constant(X, has_constant='add')\n",
    "#     else:\n",
    "#         X_sm = X\n",
    "#     model = sm.OLS(y, X_sm).fit()\n",
    "#     display(model.summary())\n",
    "\n",
    "#     # Display a QQ plot\n",
    "#     if qq:\n",
    "#         fig_qq = sm.graphics.qqplot(model.resid, line='45', fit=True,)\n",
    "#         fig_qq.suptitle('QQ plot for residual normality check')\n",
    "#     else:\n",
    "#         pass\n",
    "\n",
    "#     # Display a plot of predicted values vs. residuals\n",
    "#     if sced:    \n",
    "#         preds = model.predict(X_sm)\n",
    "#         residuals = model.resid\n",
    "#         fig_resid, ax = plt.subplots(figsize=(10,5))\n",
    "#         fig_resid.suptitle('Predicted vs. residual plot for homoscedasticity check')\n",
    "#         ax.scatter(preds, residuals, alpha=0.2)\n",
    "#         ax.plot(preds, [0 for i in range(len(X_sm))])\n",
    "#         ax.set_xlabel(\"Predicted Value\")\n",
    "#         ax.set_ylabel(\"Actual - Predicted Value\");\n",
    "#     else:\n",
    "#         pass\n",
    "#     print(f'Model adjusted R-squared: {model.rsquared_adj}')\n",
    "#     print(f'Model RMSE: {np.sqrt(model.mse_resid)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_qq_sced(y_train, X_train_combined_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total time:\n",
    "time_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Evaluation\n",
    "\n",
    "**?????????????????** had the lowest loss and highest R-squared when tested on the validation set.\n",
    "\n",
    "Finally, it's time to see how the model performs on the **test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model = ????????????\n",
    "\n",
    "# final_model.evaluate(X_test_combined_df_scaled, y_test, return_dict=True)\n",
    "# print(model_evaluation(m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total time:\n",
    "time_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's what these results mean about the final model.\n",
    "????????????????????\n",
    "<!-- * **Accuracy:** The model will correctly classify 78% of all tweets.\n",
    "* **Recall:** The model will correctly classify 72% of *actual disaster tweets*. The other 28% are false negatives.\n",
    "* **Precision:** Of all the tweets the model puts in the disaster category, 73% of them will be correct. The other 27% are false positives. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "\n",
    "### Recommendations\n",
    "<!-- * Because false negatives are still an issue, reporters should still look at all tweets, but can also be given the model's probability that a tweet is about a disaster.\n",
    "* Discard search terms that don't yield many disaster tweets, such as \"harm,\" \"bloody,\" \"screaming,\" \"ruin,\" etc.\n",
    "* Narrow the criteria for what constitutes a \"disaster.\" This dataset sometimes puts the \"disaster\" label on long-term crises like droughts, and past disasters like the hiroshima bombing. Perhaps the *The Flatiron Post* should focus on so-called \"kinetic events\" and more unpredictable crises (bombings, earthquakes, crashes, etc.). This would require either relabeling the dataset or gathering new data. -->\n",
    "\n",
    "### Limitations and Future Work\n",
    "\n",
    "<!-- * The training of this model is limited by the tweets provided, as well as the search terms that were used to obtain them. Searching for things like \"explosion,\" \"fire,\" \"suicide bomber,\" etc. seems like it should yield tweets about disasters. But there may be other tweets without blatant keywords. Having access to a less biased sample of tweets might yield better results.\n",
    "* The tweets in the provided dataset show if a tweet originally contained a URL, but not if it contained a picture or video. Having that as a feature might have improved the model's performance.\n",
    "* The purpose of this model is to provide *The Flatiron Post* with a feed-like tool that shows tweets related to disasters and crises. This model is just one piece of the pipeline. Other pieces include a tool that automatically requests tweets through Twitter's API, as well as a user-friendly interface. -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('capstone-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "741bbb06d8fece82f103e7a4762f2cad41a4667d502102b48dc88a29a56aa95e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
